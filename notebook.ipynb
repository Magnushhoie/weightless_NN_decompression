{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Decompression Without Storing Weights\n",
    "\n",
    "#### 1. Arithmetic Coding for compressing sequences of digits (i.e. 000100) based on the probability of 0/1\n",
    "Arithmetic coding is a method that compresses data by representing a sequence of symbols (like '0' and '1') as a single fractional number between 0 and 1. Instead of representing each symbol with a fixed number of bits, arithmetic coding allocates a range within that fraction based on the symbol's predicted probability.\n",
    "\n",
    "Key Points:\n",
    "- Probability Distribution: The efficiency of arithmetic coding depends on how well we understand the probability distribution of the symbols. In our case, it's the likelihood of encountering '0' or '1' in the sequence.\n",
    "- Compression Efficiency: The better we can predict the distribution (i.e., the more accurate our probabilities), the more efficient our compression will be. For example, if '0' is highly probable and we predict it correctly, it gets a larger range and thus requires fewer bits to represent.\n",
    "- Neural Networks & Compression: If we can optimally learn the probability distribution for 0/1 with a NN, we can achieve a better compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arithmetic_compressor in /Users/maghoi/opt/anaconda3/envs/py39/lib/python3.9/site-packages (0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install arithmetic_compressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: (6): 000000\n",
      "Compressed (2): 01\n",
      "Decoded (6): 000000\n",
      "Compression ratio: 3.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from arithmetic_compressor import AECompressor\n",
    "from arithmetic_compressor.models import StaticModel\n",
    "\n",
    "# Create the model, with the probability of 0 being 0.65 and 1 being 0.35 (arbitrary)\n",
    "# The better we learn the probabilities, the better the compression\n",
    "model = StaticModel({'0': 0.8, '1': 0.20})\n",
    "\n",
    "# create an arithmetic coder\n",
    "coder = AECompressor(model)\n",
    "\n",
    "# encode some data\n",
    "data = \"000000\"\n",
    "\n",
    "# Compress, convert to string\n",
    "compressed = coder.compress(data)\n",
    "compressed_str = \"\".join([str(s) for s in coder.compress(data)])\n",
    "\n",
    "# Decompress\n",
    "N = len(data)\n",
    "decoded = coder.decompress(compressed, N)\n",
    "decoded_str = \"\".join([str(s) for s in coder.decompress(compressed, N)])\n",
    "\n",
    "print(f\"Data: ({len(data)}): {data}\")\n",
    "print(f\"Compressed ({len(compressed_str)}): {compressed_str}\")\n",
    "print(f\"Decoded ({len(decoded_str)}): {decoded_str}\")\n",
    "print(f\"Compression ratio: {len(data) / len(compressed):.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network compression using an Arithmetic Coder, by learning probabilities for 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class BasicLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, fixed_value=0.1):\n",
    "        super(BasicLSTM, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # Setting fixed initial values for weights and biases\n",
    "        with torch.no_grad():\n",
    "            # For LSTM layer\n",
    "            self.lstm.weight_ih_l0.fill_(fixed_value)\n",
    "            self.lstm.weight_hh_l0.fill_(fixed_value)\n",
    "            self.lstm.bias_ih_l0.fill_(fixed_value)\n",
    "            self.lstm.bias_hh_l0.fill_(fixed_value)\n",
    "\n",
    "            # For FC layer\n",
    "            self.fc.weight.fill_(fixed_value)\n",
    "            self.fc.bias.fill_(fixed_value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.lstm.hidden_size)\n",
    "        c0 = torch.zeros(1, x.size(0), self.lstm.hidden_size)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 0, 0, 0, 0, 0], dtype=torch.int32),\n",
       " tensor([0, 0, 0, 0, 0, 1], dtype=torch.int32),\n",
       " tensor([0, 0, 0, 0, 1, 0], dtype=torch.int32),\n",
       " tensor([0, 0, 0, 0, 1, 1], dtype=torch.int32)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data file into a PyTorch tensor\n",
    "dataset = []\n",
    "with open(\"data.txt\", 'r') as in_file:\n",
    "    for line in in_file:\n",
    "        tensor_representation = torch.tensor([int(bit) for bit in line.strip()], dtype=torch.int)\n",
    "        dataset.append(tensor_representation)\n",
    "\n",
    "dataset[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[[-1., -1., -1., -1., -1., -1.]]])\n",
      "Output: tensor([[0.5000, 0.5000]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Data we want to compress\n",
    "sample = dataset[0]\n",
    "max_sentence_len = len(sample)\n",
    "\n",
    "# Network hyperparameters\n",
    "input_dim = max_sentence_len  # Max length of sentence (5 tokens)\n",
    "num_layers = 1 # One layer LSTM\n",
    "hidden_dim = 10 # Hidden dimension\n",
    "output_dim = 2 # since we have two possible outputs: 0 and 1\n",
    "learning_rate = 1e-01 # Learning rate\n",
    "\n",
    "# Create the LSTM model and optimizer\n",
    "lstm_model = BasicLSTM(input_dim, hidden_dim, output_dim)\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Test that it works\n",
    "t = torch.Tensor([-1] * input_dim).view(1,1,-1)\n",
    "print(f\"Input: {t}\")\n",
    "print(f\"Output: {lstm_model(t)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[[ 0., -1., -1., -1., -1., -1.]]])\n",
      "Output: [[0.5 0.5]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0., -1., -1., -1., -1.]]])\n",
      "Output: [[0.5132045  0.48679546]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0., -1., -1., -1.]]])\n",
      "Output: [[0.5253333  0.47466666]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0., -1., -1.]]])\n",
      "Output: [[0.5369131  0.46308687]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0.,  0., -1.]]])\n",
      "Output: [[0.5482952  0.45170477]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Original sentence: 000000\n",
      "Compessed sentence: 00001\n",
      "Compression ratio: 1.2\n",
      "\n",
      "Input: tensor([[[ 0., -1., -1., -1., -1., -1.]]])\n",
      "Output: [[0.5623161 0.4376839]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0., -1., -1., -1., -1.]]])\n",
      "Output: [[0.5735896  0.42641035]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0., -1., -1., -1.]]])\n",
      "Output: [[0.5838591  0.41614088]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0., -1., -1.]]])\n",
      "Output: [[0.5936383  0.40636164]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0.,  0., -1.]]])\n",
      "Output: [[0.60326385 0.39673617]] (probability distribution))\n",
      "Target: [1], Predicted 0\n",
      "\n",
      "Original sentence: 000001\n",
      "Compessed sentence: 00011\n",
      "Compression ratio: 1.2\n",
      "\n",
      "Input: tensor([[[ 0., -1., -1., -1., -1., -1.]]])\n",
      "Output: [[0.595491   0.40450898]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0., -1., -1., -1., -1.]]])\n",
      "Output: [[0.6050559  0.39494407]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0., -1., -1., -1.]]])\n",
      "Output: [[0.6137651  0.38623482]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0., -1., -1.]]])\n",
      "Output: [[0.6221151  0.37788492]] (probability distribution))\n",
      "Target: [1], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0.,  1., -1.]]])\n",
      "Output: [[0.60718954 0.3928104 ]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Original sentence: 000010\n",
      "Compessed sentence: 001\n",
      "Compression ratio: 2.0\n",
      "\n",
      "Original sentence: 000011\n",
      "Compessed sentence: 0011\n",
      "Compression ratio: 1.5\n",
      "\n",
      "Original sentence: 000100\n",
      "Compessed sentence: 00111\n",
      "Compression ratio: 1.2\n",
      "\n",
      "Original sentence: 000101\n",
      "Compessed sentence: 010001\n",
      "Compression ratio: 1.0\n",
      "\n",
      "Original sentence: 000110\n",
      "Compessed sentence: 01001\n",
      "Compression ratio: 1.2\n",
      "\n",
      "Original sentence: 001000\n",
      "Compessed sentence: 01011\n",
      "Compression ratio: 1.2\n",
      "\n",
      "Original sentence: 001001\n",
      "Compessed sentence: 01101\n",
      "Compression ratio: 1.2\n",
      "\n",
      "Original sentence: 001010\n",
      "Compessed sentence: 0001\n",
      "Compression ratio: 1.5\n",
      "\n",
      "Original sentence: 000000\n",
      "Compessed sentence: 001\n",
      "Compression ratio: 2.0\n",
      "\n",
      "Original sentence: 000001\n",
      "Compessed sentence: 01\n",
      "Compression ratio: 3.0\n",
      "\n",
      "Original sentence: 000010\n",
      "Compessed sentence: 01001\n",
      "Compression ratio: 1.2\n",
      "\n",
      "Original sentence: 000011\n",
      "Compessed sentence: 0101\n",
      "Compression ratio: 1.5\n",
      "\n",
      "Original sentence: 000100\n",
      "Compessed sentence: 001\n",
      "Compression ratio: 2.0\n",
      "\n",
      "Original sentence: 000000\n",
      "Compessed sentence: 01\n",
      "Compression ratio: 3.0\n",
      "\n",
      "Original sentence: 000001\n",
      "Compessed sentence: 0101\n",
      "Compression ratio: 1.5\n",
      "\n",
      "Original sentence: 000010\n",
      "Compessed sentence: 011\n",
      "Compression ratio: 2.0\n",
      "\n",
      "Original sentence: 000011\n",
      "Compessed sentence: 0111\n",
      "Compression ratio: 1.5\n",
      "\n",
      "Original sentence: 000100\n",
      "Compessed sentence: 01111\n",
      "Compression ratio: 1.2\n",
      "\n",
      "Original sentence: 000101\n",
      "Compessed sentence: 1\n",
      "Compression ratio: 6.0\n",
      "\n",
      "Original sentence: 000110\n",
      "Compessed sentence: 1\n",
      "Compression ratio: 6.0\n",
      "\n",
      "Original sentence: 001000\n",
      "Compessed sentence: 100101\n",
      "Compression ratio: 1.0\n",
      "\n",
      "Original sentence: 001001\n",
      "Compessed sentence: 100111\n",
      "Compression ratio: 1.0\n",
      "\n",
      "Original sentence: 001010\n",
      "Compessed sentence: 001\n",
      "Compression ratio: 2.0\n",
      "\n",
      "Original sentence: 000000\n",
      "Compessed sentence: 0101\n",
      "Compression ratio: 1.5\n",
      "\n",
      "Original sentence: 000001\n",
      "Compessed sentence: 0111\n",
      "Compression ratio: 1.5\n",
      "\n",
      "Original sentence: 000010\n",
      "Compessed sentence: 0111\n",
      "Compression ratio: 1.5\n",
      "\n",
      "Original sentence: 000011\n",
      "Compessed sentence: 1\n",
      "Compression ratio: 6.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_model = BasicLSTM(input_dim, hidden_dim, output_dim, fixed_value=-0.1)\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Prepare compressed text file\n",
    "with open('compressed.txt', 'w') as file:\n",
    "    # First sentence is actually not compressed\n",
    "    compressed_str = \"\".join([str(s) for s in dataset[0].numpy()])\n",
    "    file.write(f'{compressed_str}\\n')\n",
    "\n",
    "# Iterate over all sentences in dataset\n",
    "for i2, sentence in enumerate(dataset):\n",
    "\n",
    "    # Since we always compress the next sentence, we stop at the second last sentence\n",
    "    if i2 == len(dataset)-1:\n",
    "        break\n",
    "\n",
    "    # Input tensor starts with -1 (missing) for all (6) positions\n",
    "    # Gets filled one by one as we progress through the sentence\n",
    "    input_tensor = torch.Tensor([-1]*max_sentence_len).view(1, 1, -1)\n",
    "\n",
    "    # Iterate over all tokens in sentence\n",
    "    for i, bit in enumerate(sentence):\n",
    "        # Since we are predicting next token, we stop at the second last token\n",
    "        if i == len(sentence)-1:\n",
    "            break\n",
    "\n",
    "        # Input is all tokens seen so far\n",
    "        input_tensor[0, 0, i] = sentence[i].view(1, 1, 1)\n",
    "\n",
    "        # Next token to predict\n",
    "        target_tensor = sentence[i+1].view(1).long()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = lstm_model(input_tensor)\n",
    "        loss = criterion(output, target_tensor)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print stats for first iterations\n",
    "        if i < 6 and i2 < 3:\n",
    "            print(f\"Input: {input_tensor}\")\n",
    "            print(f\"Output: {output.detach().numpy()} (probability distribution))\")\n",
    "            print(f\"Target: {target_tensor.detach().numpy()}, Predicted {np.argmax(output.detach().numpy())}\\n\")\n",
    "\n",
    "    # After training on sentence, we compress it with the learned probability distribution\n",
    "    # The longer we train, the better the compression (should) be\n",
    "    prob_distribution = {\n",
    "        0: output[0, 0].item(), # Learned probability for 0\n",
    "        1: output[0, 1].item(), # Learned probability for 1\n",
    "        }\n",
    "\n",
    "    model = StaticModel(prob_distribution)\n",
    "    coder = AECompressor(model)\n",
    "\n",
    "    next_sentence = dataset[i2+1].numpy().tolist()\n",
    "    compressed = coder.compress(next_sentence)\n",
    "    compressed_str = ''.join([str(i) for i in compressed])\n",
    "    orig_str = \"\".join([str(s) for s in sentence.numpy()])\n",
    "\n",
    "    with open('compressed.txt', 'a') as file:\n",
    "        file.write(f'{compressed_str}\\n')\n",
    "\n",
    "    print(f\"Original sentence: {orig_str}\")\n",
    "    print(f\"Compessed sentence: {compressed_str}\")\n",
    "    print(f\"Compression ratio: {len(orig_str)/len(compressed_str)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size data 210, size compressed 148, compression ratio 1.4189189189189189\n"
     ]
    }
   ],
   "source": [
    "# Calculate compression ratio\n",
    "import os\n",
    "size_data, size_compressed = os.path.getsize(\"data.txt\"), os.path.getsize(\"compressed.txt\")\n",
    "print(f\"Size data {size_data}, size compressed {size_compressed}, compression ratio {size_data/size_compressed}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decompressing WITHOUT transmitting neural weights, by learning iteratively from the decompressed data\n",
    "\n",
    "The essence of this method lies in the synchronized evolution of the encoder and decoder, even when they operate independently.\n",
    "\n",
    "##### Initialization:\n",
    "- Start with a neural network decoder having all weights set to a fixed, known value.\n",
    "\n",
    "##### Kickstarting the Process:\n",
    "- Remember, the first sequence isn't compressed. Load this uncompressed sequence as the initial data.\n",
    "\n",
    "##### Iterative Learning:\n",
    "- Prediction: Use the decoder to predict the next sequence based on the current state of the neural network.\n",
    "\n",
    "- Decompression: Based on the predicted probabilities, decompress the sequence. This gives you the original data.\n",
    "\n",
    "- Network Update: Now, train the neural network using this newly decompressed sequence. This refines its predictions for subsequent sequences.\n",
    "\n",
    "##### Continuation:\n",
    "For each subsequent sequence, repeat the above iterative learning process. The decompressed sequence from the previous step helps in predicting and decompressing the next.\n",
    "\n",
    "##### Why This Works:\n",
    "Both encoder and decoder start with identical neural network setups. As sequences are processed, both learn and adjust their weights in the same manner. This synchronized evolution ensures that the decoder can accurately decompress, even without receiving the encoder's neural weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 0, 0, 0, 0, 0], dtype=torch.int32),\n",
       " tensor([0, 0, 0, 0, 1], dtype=torch.int32),\n",
       " tensor([0, 0, 0, 1, 1], dtype=torch.int32),\n",
       " tensor([0, 0, 1], dtype=torch.int32)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's decompress!\n",
    "# Load the compressed dataset into a PyTorch tensor\n",
    "compressed_dataset = []\n",
    "with open(\"compressed.txt\", 'r') as in_file:\n",
    "    for line in in_file:\n",
    "        tensor_representation = torch.tensor([int(bit) for bit in line.strip()], dtype=torch.int)\n",
    "        compressed_dataset.append(tensor_representation)\n",
    "\n",
    "compressed_dataset[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[[ 0., -1., -1., -1., -1., -1.]]])\n",
      "Output: [[0.5 0.5]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0., -1., -1., -1., -1.]]])\n",
      "Output: [[0.5132045  0.48679546]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[[ 0.,  0.,  0., -1., -1., -1.]]])\n",
      "Output: [[0.5253333  0.47466666]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0., -1., -1.]]])\n",
      "Output: [[0.5369131  0.46308687]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0.,  0., -1.]]])\n",
      "Output: [[0.5482952  0.45170477]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Original sentence: 000001\n",
      "Compressed sentence: 00001\n",
      "Decoded sentence: 000001\n",
      "\n",
      "Input: tensor([[[ 0., -1., -1., -1., -1., -1.]]])\n",
      "Output: [[0.5623161 0.4376839]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0., -1., -1., -1., -1.]]])\n",
      "Output: [[0.5735896  0.42641035]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0., -1., -1., -1.]]])\n",
      "Output: [[0.5838591  0.41614088]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0., -1., -1.]]])\n",
      "Output: [[0.5936383  0.40636164]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0.,  0., -1.]]])\n",
      "Output: [[0.60326385 0.39673617]] (probability distribution))\n",
      "Target: [1], Predicted 0\n",
      "\n",
      "Original sentence: 000010\n",
      "Compressed sentence: 00011\n",
      "Decoded sentence: 000010\n",
      "\n",
      "Input: tensor([[[ 0., -1., -1., -1., -1., -1.]]])\n",
      "Output: [[0.595491   0.40450898]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0., -1., -1., -1., -1.]]])\n",
      "Output: [[0.6050559  0.39494407]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0., -1., -1., -1.]]])\n",
      "Output: [[0.6137651  0.38623482]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0., -1., -1.]]])\n",
      "Output: [[0.6221151  0.37788492]] (probability distribution))\n",
      "Target: [1], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0.,  1., -1.]]])\n",
      "Output: [[0.60718954 0.3928104 ]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Original sentence: 000011\n",
      "Compressed sentence: 001\n",
      "Decoded sentence: 000011\n",
      "\n",
      "Original sentence: 000100\n",
      "Compressed sentence: 0011\n",
      "Decoded sentence: 000100\n",
      "\n",
      "Original sentence: 000101\n",
      "Compressed sentence: 00111\n",
      "Decoded sentence: 000101\n",
      "\n",
      "Original sentence: 000110\n",
      "Compressed sentence: 010001\n",
      "Decoded sentence: 000110\n",
      "\n",
      "Original sentence: 001000\n",
      "Compressed sentence: 01001\n",
      "Decoded sentence: 001000\n",
      "\n",
      "Original sentence: 001001\n",
      "Compressed sentence: 01011\n",
      "Decoded sentence: 001001\n",
      "\n",
      "Original sentence: 001010\n",
      "Compressed sentence: 01101\n",
      "Decoded sentence: 001010\n",
      "\n",
      "Original sentence: 000000\n",
      "Compressed sentence: 0001\n",
      "Decoded sentence: 000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize new LSTM model, but with fixed weights same as before\n",
    "# NB: No weights are loaded - we will learn them as we decompress\n",
    "lstm_model = BasicLSTM(input_dim, hidden_dim, output_dim, fixed_value=-0.1)\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Prepare compressed text file\n",
    "with open('decompressed.txt', 'w') as file:\n",
    "    # First sentence is actually not compressed\n",
    "    compressed_str = \"\".join([str(s) for s in dataset[0].numpy()])\n",
    "    file.write(f'{compressed_str}\\n')\n",
    "\n",
    "# Nb: First sentence is not compressed!\n",
    "decompressed_dataset = []\n",
    "decompressed_dataset.append(compressed_dataset[0])\n",
    "\n",
    "# Iterate over all sentences in dataset. Note that first sentence is uncompressed\n",
    "for i2, _ in enumerate(compressed_dataset):\n",
    "\n",
    "    # Grab last decompressed sentence\n",
    "    sentence = decompressed_dataset[-1]\n",
    "    \n",
    "    # Input tensor starts with -1 (missing) for all (6) positions\n",
    "    # Gets filled one by one as we progress through the sentence\n",
    "    input_tensor = torch.Tensor([-1]*max_sentence_len).view(1, 1, -1)\n",
    "\n",
    "    if i2 == len(compressed_dataset)-1:\n",
    "        break\n",
    "\n",
    "    for i, bit in enumerate(sentence):\n",
    "\n",
    "        if i == len(sentence)-1:\n",
    "            break\n",
    "\n",
    "        # Input is all tokens so far\n",
    "        input_tensor[0, 0, i] = sentence[i].view(1, 1, 1)\n",
    "\n",
    "        # Next token to predict\n",
    "        target_tensor = sentence[i+1].view(1).long()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = lstm_model(input_tensor)\n",
    "        loss = criterion(output, target_tensor)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print stats for first 10 iterations\n",
    "        if i < 7 and i2 < 3:\n",
    "            print(f\"Input: {input_tensor}\")\n",
    "            print(f\"Output: {output.detach().numpy()} (probability distribution))\")\n",
    "            print(f\"Target: {target_tensor.detach().numpy()}, Predicted {np.argmax(output.detach().numpy())}\\n\")\n",
    "\n",
    "    # After training on sentence, we used learned probability distribution to decompress the next one!\n",
    "    prob_distribution = {\n",
    "        0: output[0, 0].item(), # Learned probability for 0\n",
    "        1: output[0, 1].item() # Learned probability for 1\n",
    "        }\n",
    "\n",
    "    model = StaticModel(prob_distribution)\n",
    "    coder = AECompressor(model)\n",
    "\n",
    "    # Decompress next sentence\n",
    "    next_comp_sentence = compressed_dataset[i2+1].numpy().tolist()\n",
    "    decoded = coder.decompress(next_comp_sentence, max_sentence_len)\n",
    "    decoded_str = \"\".join([str(s) for s in decoded])\n",
    "\n",
    "    # Add decoded sentence to decompressed dataset\n",
    "    decompressed_dataset.append(torch.Tensor(decoded))\n",
    "\n",
    "    # Write to file\n",
    "    with open('decompressed.txt', 'a') as file:\n",
    "        file.write(f'{decoded_str}\\n')    \n",
    "\n",
    "    if i2 < 10:\n",
    "        compressed_str = \"\".join([str(s) for s in next_comp_sentence])\n",
    "        orig_str = \"\".join([str(s) for s in dataset[i2+1].numpy()])\n",
    "        print(f\"Original sentence: {orig_str}\")\n",
    "        print(f\"Compressed sentence: {compressed_str}\")\n",
    "        print(f\"Decoded sentence: {decoded_str}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 maghoi  staff   148B Aug 19 13:12 compressed.txt\n",
      "-rw-r--r--  1 maghoi  staff   210B Aug 18 23:42 data.txt\n",
      "-rw-r--r--  1 maghoi  staff   210B Aug 19 13:12 decompressed.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh *.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check that decompressed data is the same as the original dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import filecmp\n",
    "print(f\"Check that decompressed data is the same as the original dataset\")\n",
    "filecmp.cmp('data.txt', 'decompressed.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What we have achieved so far:\n",
    "- We trained an encoder neural network to predict the next digits in the sequences of data.txt\n",
    "- By iteratively using the learned NN encoder probability distributions, we to compress the sequences into compressed.txt using Arithmetic Coding\n",
    "- By exactly mirroring the process using the decoder network, we iteratively decompress and train again on the original sequences\n",
    "- Outcome: We losslessly decompressed all sequences in compressed.txt WITHOUT transmitting the compressor NN weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py393",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
