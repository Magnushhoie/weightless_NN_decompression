{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arithmetic_compressor in /Users/maghoi/opt/anaconda3/envs/py39/lib/python3.9/site-packages (0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install arithmetic_compressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: (6): 000000\n",
      "Compressed (4): 0001\n",
      "Decoded (6): 000000\n",
      "Compression ratio: 1.50\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from arithmetic_compressor import AECompressor\n",
    "from arithmetic_compressor.models import StaticModel\n",
    "\n",
    "# create the model\n",
    "model = StaticModel({'0': 0.65, '1': 0.35})\n",
    "\n",
    "# create an arithmetic coder\n",
    "coder = AECompressor(model)\n",
    "\n",
    "# encode some data\n",
    "data = \"000000\"\n",
    "\n",
    "# Compress, convert to string\n",
    "compressed = coder.compress(data)\n",
    "compressed_str = \"\".join([str(s) for s in coder.compress(data)])\n",
    "\n",
    "# Decompress\n",
    "N = len(data)\n",
    "decoded = coder.decompress(compressed, N)\n",
    "decoded_str = \"\".join([str(s) for s in coder.decompress(compressed, N)])\n",
    "\n",
    "print(f\"Data: ({len(data)}): {data}\")\n",
    "print(f\"Compressed ({len(compressed_str)}): {compressed_str}\")\n",
    "print(f\"Decoded ({len(decoded_str)}): {decoded_str}\")\n",
    "print(f\"Compression ratio: {len(data) / len(compressed):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class BasicLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, fixed_value=0.1):\n",
    "        super(BasicLSTM, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # Setting fixed initial values for weights and biases\n",
    "        with torch.no_grad():\n",
    "            # For LSTM layer\n",
    "            self.lstm.weight_ih_l0.fill_(fixed_value)\n",
    "            self.lstm.weight_hh_l0.fill_(fixed_value)\n",
    "            self.lstm.bias_ih_l0.fill_(fixed_value)\n",
    "            self.lstm.bias_hh_l0.fill_(fixed_value)\n",
    "\n",
    "            # For FC layer\n",
    "            self.fc.weight.fill_(fixed_value)\n",
    "            self.fc.bias.fill_(fixed_value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.lstm.hidden_size)\n",
    "        c0 = torch.zeros(1, x.size(0), self.lstm.hidden_size)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 0, 0, 0, 0, 0], dtype=torch.int32),\n",
       " tensor([0, 0, 0, 0, 0, 1], dtype=torch.int32),\n",
       " tensor([0, 0, 0, 0, 1, 0], dtype=torch.int32),\n",
       " tensor([0, 0, 0, 0, 1, 1], dtype=torch.int32)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the file into a PyTorch tensor\n",
    "dataset = []\n",
    "with open(\"data.txt\", 'r') as in_file:\n",
    "    for line in in_file:\n",
    "        tensor_representation = torch.tensor([int(bit) for bit in line.strip()], dtype=torch.int)\n",
    "        dataset.append(tensor_representation)\n",
    "\n",
    "dataset[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[[-1., -1., -1., -1., -1., -1.]]])\n",
      "Output: tensor([[0.5000, 0.5000]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Data we want to compress\n",
    "sample = dataset[0]\n",
    "max_sentence_len = len(sample)\n",
    "\n",
    "# Network hyperparameters\n",
    "input_dim = max_sentence_len  # Max length of sentence (5 tokens)\n",
    "num_layers = 1 # One layer LSTM\n",
    "hidden_dim = 5 # Hidden dimension\n",
    "output_dim = 2 # since we have two possible outputs: 0 and 1\n",
    "learning_rate = 5e-01 # Learning rate\n",
    "\n",
    "# Create the LSTM model and optimizer\n",
    "lstm_model = BasicLSTM(input_dim, hidden_dim, output_dim)\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Test that it works\n",
    "t = torch.Tensor([-1] * input_dim).view(1,1,-1)\n",
    "print(f\"Input: {t}\")\n",
    "print(f\"Output: {lstm_model(t)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[[ 0., -1., -1., -1., -1., -1.]]])\n",
      "Output: [[0.5 0.5]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0., -1., -1., -1., -1.]]])\n",
      "Output: [[0.56391734 0.43608257]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0., -1., -1., -1.]]])\n",
      "Output: [[0.6190611 0.3809389]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0., -1., -1.]]])\n",
      "Output: [[0.66563267 0.33436733]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0.,  0., -1.]]])\n",
      "Output: [[0.7046688  0.29533118]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Original sentence: 000000\n",
      "Compessed sentence: 001\n",
      "Compression ratio: 2.0\n",
      "\n",
      "Input: tensor([[[ 0., -1., -1., -1., -1., -1.]]])\n",
      "Output: [[0.74277276 0.25722724]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0., -1., -1., -1., -1.]]])\n",
      "Output: [[0.769207   0.23079298]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0., -1., -1., -1.]]])\n",
      "Output: [[0.7904353  0.20956464]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0., -1., -1.]]])\n",
      "Output: [[0.8080916  0.19190839]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0.,  0., -1.]]])\n",
      "Output: [[0.8232166  0.17678332]] (probability distribution))\n",
      "Target: [1], Predicted 0\n",
      "\n",
      "Original sentence: 000001\n",
      "Compessed sentence: 0111\n",
      "Compression ratio: 1.5\n",
      "\n",
      "Input: tensor([[[ 0., -1., -1., -1., -1., -1.]]])\n",
      "Output: [[0.80073303 0.19926701]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0., -1., -1., -1., -1.]]])\n",
      "Output: [[0.8169443  0.18305574]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0., -1., -1., -1.]]])\n",
      "Output: [[0.8302669  0.16973315]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0., -1., -1.]]])\n",
      "Output: [[0.8416465  0.15835352]] (probability distribution))\n",
      "Target: [1], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0.,  1., -1.]]])\n",
      "Output: [[0.8132908 0.1867092]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Original sentence: 000010\n",
      "Compessed sentence: 0110111\n",
      "Compression ratio: 0.8571428571428571\n",
      "\n",
      "Original sentence: 000011\n",
      "Compessed sentence: 1001\n",
      "Compression ratio: 1.5\n",
      "\n",
      "Original sentence: 000100\n",
      "Compessed sentence: 1001\n",
      "Compression ratio: 1.5\n",
      "\n",
      "Original sentence: 000101\n",
      "Compessed sentence: 101\n",
      "Compression ratio: 2.0\n",
      "\n",
      "Original sentence: 000110\n",
      "Compessed sentence: 1001\n",
      "Compression ratio: 1.5\n",
      "\n",
      "Original sentence: 001000\n",
      "Compessed sentence: 101011\n",
      "Compression ratio: 1.0\n",
      "\n",
      "Original sentence: 001001\n",
      "Compessed sentence: 101111\n",
      "Compression ratio: 1.0\n",
      "\n",
      "Original sentence: 001010\n",
      "Compessed sentence: 01\n",
      "Compression ratio: 3.0\n",
      "\n",
      "Original sentence: 000000\n",
      "Compessed sentence: 1\n",
      "Compression ratio: 6.0\n",
      "\n",
      "Original sentence: 000001\n",
      "Compessed sentence: 101\n",
      "Compression ratio: 2.0\n",
      "\n",
      "Original sentence: 000010\n",
      "Compessed sentence: 1010011\n",
      "Compression ratio: 0.8571428571428571\n",
      "\n",
      "Original sentence: 000011\n",
      "Compessed sentence: 1011\n",
      "Compression ratio: 1.5\n",
      "\n",
      "Original sentence: 000100\n",
      "Compessed sentence: 1\n",
      "Compression ratio: 6.0\n",
      "\n",
      "Original sentence: 000000\n",
      "Compessed sentence: 101\n",
      "Compression ratio: 2.0\n",
      "\n",
      "Original sentence: 000001\n",
      "Compessed sentence: 11\n",
      "Compression ratio: 3.0\n",
      "\n",
      "Original sentence: 000010\n",
      "Compessed sentence: 10111101\n",
      "Compression ratio: 0.75\n",
      "\n",
      "Original sentence: 000011\n",
      "Compessed sentence: 11001\n",
      "Compression ratio: 1.2\n",
      "\n",
      "Original sentence: 000100\n",
      "Compessed sentence: 1100101\n",
      "Compression ratio: 0.8571428571428571\n",
      "\n",
      "Original sentence: 000101\n",
      "Compessed sentence: 1100111\n",
      "Compression ratio: 0.8571428571428571\n",
      "\n",
      "Original sentence: 000110\n",
      "Compessed sentence: 1101\n",
      "Compression ratio: 1.5\n",
      "\n",
      "Original sentence: 001000\n",
      "Compessed sentence: 11010111\n",
      "Compression ratio: 0.75\n",
      "\n",
      "Original sentence: 001001\n",
      "Compessed sentence: 11011011\n",
      "Compression ratio: 0.75\n",
      "\n",
      "Original sentence: 001010\n",
      "Compessed sentence: 1\n",
      "Compression ratio: 6.0\n",
      "\n",
      "Original sentence: 000000\n",
      "Compessed sentence: 1011\n",
      "Compression ratio: 1.5\n",
      "\n",
      "Original sentence: 000001\n",
      "Compessed sentence: 11\n",
      "Compression ratio: 3.0\n",
      "\n",
      "Original sentence: 000010\n",
      "Compessed sentence: 11000011\n",
      "Compression ratio: 0.75\n",
      "\n",
      "Original sentence: 000011\n",
      "Compessed sentence: 1101\n",
      "Compression ratio: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_model = BasicLSTM(input_dim, hidden_dim, output_dim, fixed_value=-0.1)\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Prepare compressed text file\n",
    "with open('compressed.txt', 'w') as file:\n",
    "    # First sentence is actually not compressed\n",
    "    compressed_str = \"\".join([str(s) for s in dataset[0].numpy()])\n",
    "    file.write(f'{compressed_str}\\n')\n",
    "\n",
    "# Iterate over all sentences in dataset\n",
    "for i2, sentence in enumerate(dataset):\n",
    "\n",
    "    # We don't want to train on the last sentence, as we will already have compressed\n",
    "    if i2 == len(dataset)-1:\n",
    "        break\n",
    "    \n",
    "    # Input tensor starts with -1 (missing) for all (6) positions\n",
    "    # Gets filled one by one as we progress through the sentence\n",
    "    input_tensor = torch.Tensor([-1]*max_sentence_len).view(1, 1, -1)\n",
    "\n",
    "    for i, bit in enumerate(sentence):\n",
    "        if i == len(sentence)-1:\n",
    "            break\n",
    "\n",
    "        # Input is all tokens so far\n",
    "        input_tensor[0, 0, i] = sentence[i].view(1, 1, 1)\n",
    "\n",
    "        # Next token to predict\n",
    "        target_tensor = sentence[i+1].view(1).long()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = lstm_model(input_tensor)\n",
    "        loss = criterion(output, target_tensor)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print stats for first 10 iterations\n",
    "        if i < 6 and i2 < 3:\n",
    "            print(f\"Input: {input_tensor}\")\n",
    "            print(f\"Output: {output.detach().numpy()} (probability distribution))\")\n",
    "            print(f\"Target: {target_tensor.detach().numpy()}, Predicted {np.argmax(output.detach().numpy())}\\n\")\n",
    "\n",
    "    # After training on sentence, we compress it with the learned probability distribution\n",
    "    prob_distribution = {\n",
    "        0: output[0, 0].item(), # Learned probability for 0\n",
    "        1: output[0, 1].item() # Learned probability for 1\n",
    "        }\n",
    "\n",
    "    model = StaticModel(prob_distribution)\n",
    "    coder = AECompressor(model)\n",
    "\n",
    "    next_sentence = dataset[i2+1].numpy().tolist()\n",
    "    compressed = coder.compress(next_sentence)\n",
    "    compressed_str = ''.join([str(i) for i in compressed])\n",
    "    orig_str = \"\".join([str(s) for s in sentence.numpy()])\n",
    "\n",
    "    with open('compressed.txt', 'a') as file:\n",
    "        file.write(f'{compressed_str}\\n')\n",
    "\n",
    "    print(f\"Original sentence: {orig_str}\")\n",
    "    print(f\"Compessed sentence: {compressed_str}\")\n",
    "    print(f\"Compression ratio: {len(orig_str)/len(compressed_str)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 0, 0, 0, 0, 0], dtype=torch.int32),\n",
       " tensor([0, 0, 1], dtype=torch.int32),\n",
       " tensor([0, 1, 1, 1], dtype=torch.int32),\n",
       " tensor([0, 1, 1, 0, 1, 1, 1], dtype=torch.int32)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the compressed dataset into a PyTorch tensor\n",
    "compressed_dataset = []\n",
    "with open(\"compressed.txt\", 'r') as in_file:\n",
    "    for line in in_file:\n",
    "        tensor_representation = torch.tensor([int(bit) for bit in line.strip()], dtype=torch.int)\n",
    "        compressed_dataset.append(tensor_representation)\n",
    "\n",
    "compressed_dataset[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[[ 0., -1., -1., -1., -1., -1.]]])\n",
      "Output: [[0.5 0.5]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0., -1., -1., -1., -1.]]])\n",
      "Output: [[0.56391734 0.43608257]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0., -1., -1., -1.]]])\n",
      "Output: [[0.6190611 0.3809389]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0., -1., -1.]]])\n",
      "Output: [[0.66563267 0.33436733]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0.,  0., -1.]]])\n",
      "Output: [[0.7046688  0.29533118]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Original sentence: 000001\n",
      "Compressed sentence: 001\n",
      "Decoded sentence: 000001\n",
      "\n",
      "Input: tensor([[[ 0., -1., -1., -1., -1., -1.]]])\n",
      "Output: [[0.74277276 0.25722724]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0., -1., -1., -1., -1.]]])\n",
      "Output: [[0.769207   0.23079298]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0., -1., -1., -1.]]])\n",
      "Output: [[0.7904353  0.20956464]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0., -1., -1.]]])\n",
      "Output: [[0.8080916  0.19190839]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0.,  0., -1.]]])\n",
      "Output: [[0.8232166  0.17678332]] (probability distribution))\n",
      "Target: [1], Predicted 0\n",
      "\n",
      "Original sentence: 000010\n",
      "Compressed sentence: 0111\n",
      "Decoded sentence: 000010\n",
      "\n",
      "Input: tensor([[[ 0., -1., -1., -1., -1., -1.]]])\n",
      "Output: [[0.80073303 0.19926701]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0., -1., -1., -1., -1.]]])\n",
      "Output: [[0.8169443  0.18305574]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0., -1., -1., -1.]]])\n",
      "Output: [[0.8302669  0.16973315]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0., -1., -1.]]])\n",
      "Output: [[0.8416465  0.15835352]] (probability distribution))\n",
      "Target: [1], Predicted 0\n",
      "\n",
      "Input: tensor([[[ 0.,  0.,  0.,  0.,  1., -1.]]])\n",
      "Output: [[0.8132908 0.1867092]] (probability distribution))\n",
      "Target: [0], Predicted 0\n",
      "\n",
      "Original sentence: 000011\n",
      "Compressed sentence: 0110111\n",
      "Decoded sentence: 000011\n",
      "\n",
      "Original sentence: 000100\n",
      "Compressed sentence: 1001\n",
      "Decoded sentence: 000100\n",
      "\n",
      "Original sentence: 000101\n",
      "Compressed sentence: 1001\n",
      "Decoded sentence: 000101\n",
      "\n",
      "Original sentence: 000110\n",
      "Compressed sentence: 101\n",
      "Decoded sentence: 000110\n",
      "\n",
      "Original sentence: 001000\n",
      "Compressed sentence: 1001\n",
      "Decoded sentence: 001000\n",
      "\n",
      "Original sentence: 001001\n",
      "Compressed sentence: 101011\n",
      "Decoded sentence: 001001\n",
      "\n",
      "Original sentence: 001010\n",
      "Compressed sentence: 101111\n",
      "Decoded sentence: 001010\n",
      "\n",
      "Original sentence: 000000\n",
      "Compressed sentence: 01\n",
      "Decoded sentence: 000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize new LSTM model, but with fixed weights same as before\n",
    "# NB: No weights are loaded - we will learn them as we decompress\n",
    "\n",
    "lstm_model = BasicLSTM(input_dim, hidden_dim, output_dim, fixed_value=-0.1)\n",
    "optimizer = optim.SGD(lstm_model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Prepare compressed text file\n",
    "with open('decompressed.txt', 'w') as file:\n",
    "    # First sentence is actually not compressed\n",
    "    compressed_str = \"\".join([str(s) for s in dataset[0].numpy()])\n",
    "    file.write(f'{compressed_str}\\n')\n",
    "\n",
    "# Nb: First sentence is not compressed!\n",
    "decompressed_dataset = []\n",
    "decompressed_dataset.append(compressed_dataset[0])\n",
    "\n",
    "# Iterate over all sentences in dataset. Note that first sentence is uncompressed\n",
    "for i2, _ in enumerate(compressed_dataset):\n",
    "\n",
    "    # Grab last decompressed sentence\n",
    "    sentence = decompressed_dataset[-1]\n",
    "    \n",
    "    # Input tensor starts with -1 (missing) for all (6) positions\n",
    "    # Gets filled one by one as we progress through the sentence\n",
    "    input_tensor = torch.Tensor([-1]*max_sentence_len).view(1, 1, -1)\n",
    "\n",
    "    if i2 == len(compressed_dataset)-2:\n",
    "        break\n",
    "\n",
    "    for i, bit in enumerate(sentence):\n",
    "\n",
    "        if i == len(sentence)-1:\n",
    "            break\n",
    "\n",
    "        # Input is all tokens so far\n",
    "        input_tensor[0, 0, i] = sentence[i].view(1, 1, 1)\n",
    "\n",
    "        # Next token to predict\n",
    "        target_tensor = sentence[i+1].view(1).long()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = lstm_model(input_tensor)\n",
    "        loss = criterion(output, target_tensor)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print stats for first 10 iterations\n",
    "        if i < 7 and i2 < 3:\n",
    "            print(f\"Input: {input_tensor}\")\n",
    "            print(f\"Output: {output.detach().numpy()} (probability distribution))\")\n",
    "            print(f\"Target: {target_tensor.detach().numpy()}, Predicted {np.argmax(output.detach().numpy())}\\n\")\n",
    "\n",
    "    # After training on sentence, we used learned probability distribution to decompress the next one!\n",
    "    prob_distribution = {\n",
    "        0: output[0, 0].item(), # Learned probability for 0\n",
    "        1: output[0, 1].item() # Learned probability for 1\n",
    "        }\n",
    "\n",
    "    model = StaticModel(prob_distribution)\n",
    "    coder = AECompressor(model)\n",
    "\n",
    "    # Decompress next sentence\n",
    "    next_comp_sentence = compressed_dataset[i2+1].numpy().tolist()\n",
    "    decoded = coder.decompress(next_comp_sentence, max_sentence_len)\n",
    "    decoded_str = \"\".join([str(s) for s in decoded])\n",
    "\n",
    "    # Add decoded sentence to decompressed dataset\n",
    "    decompressed_dataset.append(torch.Tensor(decoded))\n",
    "\n",
    "    # Write to file\n",
    "    with open('decompressed.txt', 'a') as file:\n",
    "        file.write(f'{decoded_str}\\n')    \n",
    "\n",
    "    if i2 < 10:\n",
    "        compressed_str = \"\".join([str(s) for s in next_comp_sentence])\n",
    "        orig_str = \"\".join([str(s) for s in dataset[i2+1].numpy()])\n",
    "        print(f\"Original sentence: {orig_str}\")\n",
    "        print(f\"Compressed sentence: {compressed_str}\")\n",
    "        print(f\"Decoded sentence: {decoded_str}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py393",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
